# Pseudocode / runnable outline
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score, f1_score
from sklearn.utils import resample
import tensorflow as tf

# 1) Prepare a metadata table linking each sample to its source:
# columns: ['sample_id','source']  source in {'mimic','coswara','local'}
# and load input features / labels for each sample (X_* and y arrays or generators)

# Example: assemble per-sample entries
public_sources = ['mimic','coswara']
local_source = 'local'

def run_cv(X, y, n_splits=5, seed=42):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    metrics = []
    for train_idx, test_idx in skf.split(X, y):
        # Build/compile fresh model for each fold (same architecture/hyperparams)
        model = build_model()  # your Keras model
        model.fit(X[train_idx], y[train_idx], epochs=E, batch_size=B, validation_split=0.1, verbose=0)
        probs = model.predict(X[test_idx])[:,1]  # probability for positive/high-risk class
        preds = (probs >= 0.5).astype(int)
        auc = roc_auc_score(y[test_idx], probs)
        acc = accuracy_score(y[test_idx], preds)
        sens = recall_score(y[test_idx], preds, pos_label=1)
        spec = recall_score(y[test_idx], preds, pos_label=0)
        f1 = f1_score(y[test_idx], preds)
        metrics.append({'auc':auc,'acc':acc,'sens':sens,'spec':spec,'f1':f1})
    return pd.DataFrame(metrics)

# 2) Scenario datasets
# Scenario A (public-only): select rows where source in public_sources
X_pub = X[meta['source'].isin(public_sources)]
y_pub = y[meta['source'].isin(public_sources)]

# Scenario B (local-only)
X_loc = X[meta['source']==local_source]
y_loc = y[meta['source']==local_source]

# Scenario C (combined)
X_all = X
y_all = y

# 3) Run CV
res_pub = run_cv(X_pub, y_pub)
res_loc = run_cv(X_loc, y_loc)
res_all = run_cv(X_all, y_all)

# 4) Bootstrap AUC CI function
def bootstrap_auc_ci(y_true, y_prob, n_boot=1000, alpha=0.05):
    aucs = []
    n = len(y_true)
    for i in range(n_boot):
        idx = np.random.choice(n, n, replace=True)
        try:
            aucs.append(roc_auc_score(y_true[idx], y_prob[idx]))
        except:
            pass
    lower = np.percentile(aucs, 100*alpha/2)
    upper = np.percentile(aucs, 100*(1-alpha/2))
    return lower, upper

# 5) Cross-domain tests (train public -> test local)
# train on all public samples, validate/train using CV on public, then evaluate final model on local holdout
